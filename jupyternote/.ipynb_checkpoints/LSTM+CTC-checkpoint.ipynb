{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learning parameters\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#network parameters\n",
    "input_size = 50\n",
    "hidden_neuron = 128\n",
    "layer_num = 2\n",
    "num_classes = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data \n",
    "num_epochs = \n",
    "num_train_samples = \n",
    "num_val_samples = \n",
    "num_batches_per_epoch = \n",
    "num_batches_per_epoch_val = \n",
    "shuffle_idx_val = np.random.permutation(num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define graph\n",
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    #input\n",
    "    inputs = tf.placeholder(tf.float32,[batch_size, None, input_size])\n",
    "    labels = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    #build LSTM\n",
    "    stack = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(hidden_neuron, state_is_tuple = True) \n",
    "                                         for i in range(layer_num)], \n",
    "                                        state_is_tuple = True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype = tf.float32)\n",
    "    \n",
    "    #ctc process\n",
    "    in_shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = in_shape[0], in_shape[1]\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_neuron])\n",
    "    \n",
    "    w = tf.get_variable(name = 'w',\n",
    "                       shape = [hidden_neuron, num_classes],\n",
    "                       dtype = tf.float32,\n",
    "                       initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b = tf.get_variable(name = 'b',\n",
    "                       shape = [num_classes],\n",
    "                       dtype = tf.float32,\n",
    "                       initializer = tf.constant_initializer())\n",
    "    \n",
    "    logits = tf.matmul(outputs, w) + b\n",
    "    \n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, [1, 0, 2])\n",
    "    \n",
    "    #loss\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(labels = labels, inputs = logits, sequence_length = seq_len)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate,\n",
    "                                      beta1 = beta1,\n",
    "                                      beta2 = beta2).minimize(loss,\n",
    "                                                              global_step = global_step)\n",
    "    \n",
    "    #ctc decoder\n",
    "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated = False)\n",
    "    \n",
    "    dense_decoded = tf.sparse_tensor_to_dense(decoded[0], default_value = -1)\n",
    "    \n",
    "    #error rate\n",
    "    label_error_rate = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "with tf.device('/cpu:0'):\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.session(config = config) as sess:\n",
    "        #initialize\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "         print('=============================begin training=============================')\n",
    "            for cur_epoch in range(num_epochs):\n",
    "                shuffle_idx = np.random.permutation(num_train_samples)\n",
    "                train_cost = 0\n",
    "                start_time = time.time()\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # the tracing part\n",
    "                for cur_batch in range(num_batches_per_epoch):\n",
    "                    if (cur_batch + 1) % 100 == 0:\n",
    "                        print('batch', cur_batch, ': time', time.time() - batch_time)\n",
    "                    batch_time = time.time()\n",
    "                    indexs = [shuffle_idx[i % num_train_samples] for i in\n",
    "                              range(cur_batch * batch_size, (cur_batch + 1) * batch_size)]\n",
    "                    batch_inputs, batch_seq_len, batch_labels = \\\n",
    "                        train_feeder.input_index_generate_batch(indexs)\n",
    "                    # batch_inputs,batch_seq_len,batch_labels=utils.gen_batch(FLAGS.batch_size)\n",
    "                    feed = {model.inputs: batch_inputs,\n",
    "                            model.labels: batch_labels,\n",
    "                            model.seq_len: batch_seq_len}\n",
    "\n",
    "                    # if summary is needed\n",
    "                    # batch_cost,step,train_summary,_ = sess.run([cost,global_step,merged_summay,optimizer],feed)\n",
    "\n",
    "                    summary_str, batch_cost, step, _ = \\\n",
    "                        sess.run([model.merged_summay, model.cost, model.global_step,\n",
    "                                  model.train_op], feed)\n",
    "                    # calculate the cost\n",
    "                    train_cost += batch_cost * batch_size\n",
    "\n",
    "                    train_writer.add_summary(summary_str, step)\n",
    "\n",
    "                    # save the checkpoint\n",
    "                    #if step % FLAGS.save_steps == 1:\n",
    "                    #    if not os.path.isdir(FLAGS.checkpoint_dir):\n",
    "                    #        os.mkdir(FLAGS.checkpoint_dir)\n",
    "                    #    logger.info('save the checkpoint of{0}', format(step))\n",
    "                    #    saver.save(sess, os.path.join(FLAGS.checkpoint_dir, 'ocr-model'),\n",
    "                    #               global_step=step)\n",
    "\n",
    "                    # train_err += the_err * FLAGS.batch_size\n",
    "                    # do validation\n",
    "                    if step % FLAGS.validation_steps == 0:\n",
    "                        acc_batch_total = 0\n",
    "                        lastbatch_err = 0\n",
    "                        lr = 0\n",
    "                        for j in xrange(num_batches_per_epoch_val):\n",
    "                            indexs_val = [shuffle_idx_val[i % num_val_samples] for i in\n",
    "                                          range(j * batch_size, (j + 1) * batch_size)]\n",
    "                            val_inputs, val_seq_len, val_labels = \\\n",
    "                                val_feeder.input_index_generate_batch(indexs_val)\n",
    "                            val_feed = {model.inputs: val_inputs,\n",
    "                                        model.labels: val_labels,\n",
    "                                        model.seq_len: val_seq_len}\n",
    "\n",
    "                            dense_decoded, lastbatch_err, lr = \\\n",
    "                                sess.run([model.dense_decoded, model.lrn_rate],\n",
    "                                         val_feed)\n",
    "\n",
    "                            # print the decode result\n",
    "                            ori_labels = val_feeder.the_label(indexs_val)\n",
    "                            acc = utils.accuracy_calculation(ori_labels, dense_decoded,\n",
    "                                                             ignore_value=-1, isPrint=True)\n",
    "                            acc_batch_total += acc\n",
    "\n",
    "                        accuracy = (acc_batch_total * batch_size) / num_val_samples\n",
    "\n",
    "                        avg_train_cost = train_cost / ((cur_batch + 1) * batch_size)\n",
    "\n",
    "                        # train_err /= num_train_samples\n",
    "                        now = datetime.datetime.now()\n",
    "                        log = \"{}/{} {}:{}:{} Epoch {}/{}, \" \\\n",
    "                              \"accuracy = {:.3f},avg_train_cost = {:.3f}, \" \\\n",
    "                              \"lastbatch_err = {:.3f}, time = {:.3f},lr={:.8f}\"\n",
    "                        print(log.format(now.month, now.day, now.hour, now.minute, now.second,\n",
    "                                         cur_epoch + 1, epochs, accuracy, avg_train_cost,\n",
    "                                         lastbatch_err, time.time() - start_time, lr))\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
